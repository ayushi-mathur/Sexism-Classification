{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import math\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('./../sexism-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=data[data['scores']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Trying to convince Adam to make me some flower...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Women have to work much harder to make it in t...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Whenever my daughter and I talk about marriage...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>'chef Heston Blumenthal has claimed that femal...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Ushna shah the Pakistani superstar caught hers...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5332</td>\n",
       "      <td>SLUT How would you define the word? This four ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5336</td>\n",
       "      <td>Slut shaming can happen to anyone, whether the...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5431</td>\n",
       "      <td>Bloody men are like buses- You wait for about ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5462</td>\n",
       "      <td>To the men that tell women to smile more, ligh...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5503</td>\n",
       "      <td>What is a slut? Pause that thought, because it...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texts  scores  class\n",
       "3     Trying to convince Adam to make me some flower...       1      2\n",
       "7     Women have to work much harder to make it in t...       1      2\n",
       "9     Whenever my daughter and I talk about marriage...       1      2\n",
       "10    'chef Heston Blumenthal has claimed that femal...       1      1\n",
       "20    Ushna shah the Pakistani superstar caught hers...       1      2\n",
       "...                                                 ...     ...    ...\n",
       "5332  SLUT How would you define the word? This four ...       1      2\n",
       "5336  Slut shaming can happen to anyone, whether the...       1      2\n",
       "5431  Bloody men are like buses- You wait for about ...       1      2\n",
       "5462  To the men that tell women to smile more, ligh...       1      2\n",
       "5503  What is a slut? Pause that thought, because it...       1      2\n",
       "\n",
       "[403 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data=data[data['scores']==0]\n",
    "temp_data=temp_data[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=new_data.append(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So begin today. Each criticism that you face, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>It's Sunday. Y'all dirty minded people should ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Whoever decided the phrase smash your backdoor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Trying to convince Adam to make me some flower...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i wore this dress last year but now its too sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5539</td>\n",
       "      <td>Follow us for more Feminist stories &amp; quotes. . .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>rich white kids - - - - - - - - - - sivememes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5541</td>\n",
       "      <td>Got rid of the drabby brown and those shiney r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5542</td>\n",
       "      <td>piggy longevily starred at me twice as I passe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5543</td>\n",
       "      <td>URGENT FUN? Swipe left! . . . . .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texts  scores  class\n",
       "0     So begin today. Each criticism that you face, ...       0      0\n",
       "1     It's Sunday. Y'all dirty minded people should ...       0      0\n",
       "2     Whoever decided the phrase smash your backdoor...       0      0\n",
       "3     Trying to convince Adam to make me some flower...       1      2\n",
       "4     i wore this dress last year but now its too sh...       0      0\n",
       "...                                                 ...     ...    ...\n",
       "5539  Follow us for more Feminist stories & quotes. . .       0      0\n",
       "5540      rich white kids - - - - - - - - - - sivememes       0      0\n",
       "5541  Got rid of the drabby brown and those shiney r...       0      0\n",
       "5542  piggy longevily starred at me twice as I passe...       0      0\n",
       "5543                  URGENT FUN? Swipe left! . . . . .       0      0\n",
       "\n",
       "[5544 rows x 3 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "def tokenizeText(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def sent2idx(split_text):\n",
    "    sent2idx = []\n",
    "    for w in split_text:\n",
    "        if w.lower() in word2idx:\n",
    "            sent2idx.append(word2idx[w.lower()])\n",
    "        else:\n",
    "            sent2idx.append(word2idx['_UNK'])\n",
    "            \n",
    "    return sent2idx\n",
    "\n",
    "def processTextData(df,isTrain):\n",
    "    global words\n",
    "    global word2idx\n",
    "    global idx2word\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['tokenized'] = df.texts.apply(lambda x: (tokenizeText(x.lower())))\n",
    "    \n",
    "    if isTrain:\n",
    "        for sent in tqdm(df.tokenized.values):\n",
    "            words.update(w for w in sent)\n",
    "\n",
    "        words = sorted(words, key=words.get, reverse=True)\n",
    "        words = ['_PAD','_UNK'] + words\n",
    "\n",
    "        word2idx = {o:i for i,o in enumerate(words)}\n",
    "        idx2word = {i:o for i,o in enumerate(words)}\n",
    "        \n",
    "    df['vectorized'] = df.texts.apply(lambda x: sent2idx(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4158/4158 [00:00<00:00, 80125.68it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = processTextData(train_data,True)\n",
    "test_data = processTextData(test_data,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(score):\n",
    "    l=[0,0]\n",
    "    l[score]=1\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label']=train_data['scores'].apply(label)\n",
    "test_data['label']=test_data['scores'].apply(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>scores</th>\n",
       "      <th>class</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>vectorized</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4449</td>\n",
       "      <td>If youre not calling yourself a feminist, aka ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, youre, not, calling, yourself, a, feminis...</td>\n",
       "      <td>[8, 2616, 1, 1118, 1070, 445, 1115, 690, 1, 81...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4901</td>\n",
       "      <td>So, your neighbor was left to raise kids all b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, ,, your, neighbor, was, left, to, raise, ...</td>\n",
       "      <td>[373, 1070, 3, 1, 1118, 1070, 445, 1115, 1, 81...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>I dress in revealing clothes, I post 'sexy' se...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[i, dress, in, revealing, clothes, ,, i, post,...</td>\n",
       "      <td>[8, 1, 1469, 1115, 690, 373, 373, 1, 8, 812, 1...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5338</td>\n",
       "      <td>Mommy Shark, Daddy Shark, Baby Sharks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[mommy, shark, ,, daddy, shark, ,, baby, sharks]</td>\n",
       "      <td>[966, 1070, 966, 966, 1118, 1, 373, 2674, 7, 1...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4659</td>\n",
       "      <td>YouTuber, author &amp; upcoming late night TV host...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[youtuber, ,, author, &amp;, upcoming, late, night...</td>\n",
       "      <td>[1118, 1070, 445, 1090, 445, 1958, 690, 1115, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2787</td>\n",
       "      <td>GbmtiF GU]RWk gIHSLw ]u[LJC tpe`VQ ryfhJH \\upP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[gbmtif, gu, ], rwk, gihslw, ], u, [, ljc, tpe...</td>\n",
       "      <td>[4253, 1958, 966, 1090, 8, 2616, 1, 4253, 445,...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>Working to shine lights on Webs of patriarchy....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[working, to, shine, lights, on, webs, of, pat...</td>\n",
       "      <td>[6370, 1070, 1115, 3176, 8, 812, 4253, 1, 1090...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2355</td>\n",
       "      <td>Mark your calendar for Dare You Take a Walk in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[mark, your, calendar, for, dare, you, take, a...</td>\n",
       "      <td>[966, 7, 1115, 3176, 1, 1118, 1070, 445, 1115,...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2229</td>\n",
       "      <td>My face when I leave school in less than month...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, face, when, i, leave, school, in, less, t...</td>\n",
       "      <td>[966, 1118, 1, 2616, 7, 1415, 690, 1, 6370, 26...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>3/3 I felt that there was only me and a great ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3/3, i, felt, that, there, was, only, me, and...</td>\n",
       "      <td>[469, 544, 469, 1, 8, 1, 2616, 690, 926, 1090,...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4158 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  texts  scores  class  \\\n",
       "4449  If youre not calling yourself a feminist, aka ...       0      0   \n",
       "4901  So, your neighbor was left to raise kids all b...       0      0   \n",
       "1640  I dress in revealing clothes, I post 'sexy' se...       1      2   \n",
       "5338              Mommy Shark, Daddy Shark, Baby Sharks       0      0   \n",
       "4659  YouTuber, author & upcoming late night TV host...       0      0   \n",
       "...                                                 ...     ...    ...   \n",
       "2787  GbmtiF GU]RWk gIHSLw ]u[LJC tpe`VQ ryfhJH \\upP...       0      0   \n",
       "1369  Working to shine lights on Webs of patriarchy....       0      0   \n",
       "2355  Mark your calendar for Dare You Take a Walk in...       0      0   \n",
       "2229  My face when I leave school in less than month...       0      0   \n",
       "1360  3/3 I felt that there was only me and a great ...       0      0   \n",
       "\n",
       "                                              tokenized  \\\n",
       "4449  [if, youre, not, calling, yourself, a, feminis...   \n",
       "4901  [so, ,, your, neighbor, was, left, to, raise, ...   \n",
       "1640  [i, dress, in, revealing, clothes, ,, i, post,...   \n",
       "5338   [mommy, shark, ,, daddy, shark, ,, baby, sharks]   \n",
       "4659  [youtuber, ,, author, &, upcoming, late, night...   \n",
       "...                                                 ...   \n",
       "2787  [gbmtif, gu, ], rwk, gihslw, ], u, [, ljc, tpe...   \n",
       "1369  [working, to, shine, lights, on, webs, of, pat...   \n",
       "2355  [mark, your, calendar, for, dare, you, take, a...   \n",
       "2229  [my, face, when, i, leave, school, in, less, t...   \n",
       "1360  [3/3, i, felt, that, there, was, only, me, and...   \n",
       "\n",
       "                                             vectorized   label  \n",
       "4449  [8, 2616, 1, 1118, 1070, 445, 1115, 690, 1, 81...  [1, 0]  \n",
       "4901  [373, 1070, 3, 1, 1118, 1070, 445, 1115, 1, 81...  [1, 0]  \n",
       "1640  [8, 1, 1469, 1115, 690, 373, 373, 1, 8, 812, 1...  [0, 1]  \n",
       "5338  [966, 1070, 966, 966, 1118, 1, 373, 2674, 7, 1...  [1, 0]  \n",
       "4659  [1118, 1070, 445, 1090, 445, 1958, 690, 1115, ...  [1, 0]  \n",
       "...                                                 ...     ...  \n",
       "2787  [4253, 1958, 966, 1090, 8, 2616, 1, 4253, 445,...  [1, 0]  \n",
       "1369  [6370, 1070, 1115, 3176, 8, 812, 4253, 1, 1090...  [1, 0]  \n",
       "2355  [966, 7, 1115, 3176, 1, 1118, 1070, 445, 1115,...  [1, 0]  \n",
       "2229  [966, 1118, 1, 2616, 7, 1415, 690, 1, 6370, 26...  [1, 0]  \n",
       "1360  [469, 544, 469, 1, 8, 1, 2616, 690, 926, 1090,...  [1, 0]  \n",
       "\n",
       "[4158 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, maxlen=10):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "        self.df['text_padded'] = self.df.vectorized.apply(lambda x: self.pad_data(x))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.text_padded.values[idx]\n",
    "        sexism_label = self.df.label.values[idx]\n",
    "        sexism_type = self.df['class'].values[idx]\n",
    "        return text,sexism_label,sexism_type\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = VectorizeData(train_data)\n",
    "testDataset = VectorizeData(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(dataset=trainDataset, batch_size=100, shuffle=True)\n",
    "testLoader = DataLoader(dataset=testDataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "for i, samples in enumerate(trainLoader):\n",
    "    print(i)\n",
    "    print(samples[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentence to model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(s,maxlen):\n",
    "        padded = np.zeros((maxlen,), dtype=np.int64)\n",
    "        if len(s) > maxlen: padded[:] = s[:maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded\n",
    "\n",
    "def sentToTensor(text,word2idx,vectors):    \n",
    "    padded_vector = pad_data(sent2idx(tokenizeText(text)),10)\n",
    "    \n",
    "    return torch.tensor(padded_vector).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolating to MultiClass Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeDataMultiClass(Dataset):\n",
    "    def __init__(self, df, maxlen=10):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "        self.df['text_padded'] = self.df.vectorized.apply(lambda x: self.pad_data(x))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.text_padded.values[idx]\n",
    "        sexism_label = self.df.scores.values[idx]\n",
    "        sexism_type = self.df['class'].values[idx]\n",
    "        \n",
    "        if sexism_label == 0 and sexism_type == 0:\n",
    "            return text,0\n",
    "        if sexism_label == 1 and sexism_type == 1:\n",
    "            return text,1\n",
    "        if sexism_label == 1 and sexism_type == 2:\n",
    "            return text,2\n",
    "            \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasetMC = VectorizeDataMultiClass(train_data)\n",
    "testDatasetMC = VectorizeDataMultiClass(test_data)\n",
    "trainLoaderMC = DataLoader(dataset=trainDatasetMC, batch_size=100, shuffle=True)\n",
    "testLoaderMC = DataLoader(dataset=testDatasetMC, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass data\n"
     ]
    }
   ],
   "source": [
    "print('Multiclass data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 6370,  2674,  1070,  1070,  1941,   373,     2,     1,     7,   373],\n",
      "        [    8,  2616,     1,  1118,  1070,   445,  1115,   690,     1,  1070],\n",
      "        [16686,   445,   373,  1090,     1,  1469,  1115,  1070,  1941,  1941],\n",
      "        [ 2674,     7,  3683,     8,   812,  4253,     1,  1090,  2674,   690],\n",
      "        [  812,  1070,     1,  1090,  1115,   445,  1415,  3176,   690,  1115],\n",
      "        [ 3176,     7,  2674,     7,     7,   812,     1,  2674,   445,   966],\n",
      "        [    8,     1,  6370,     7,   373,     1,  2616,  1070,  1115,     1],\n",
      "        [    8,  1090,   373,     1,  1941,  1070,   373,   373,     8,  1958],\n",
      "        [ 1415,  2674,   690,  1415,  3176,     1,  1070,   445,  1090,     1],\n",
      "        [ 1070,   812,     1,  1090,  2674,     8,   373,     1,  6370,   690],\n",
      "        [  926,   690,  1090,   171,   373,     1,  2616,     8,   812,  1469],\n",
      "        [ 2674,   690,     7,  1115,  1090,  1958,  1115,   690,     7,  3176],\n",
      "        [    8,     1,   926,     8,  3683,   690,     1,  1958,  1118,     1],\n",
      "        [ 2674,   690,     1,  1090,  2674,   690,     1,  1115,   690,     7],\n",
      "        [ 1090,  2674,   690,     1,  1958,  1070,  1469,  1118,     1,  3176],\n",
      "        [ 1118,   445,   966,     3,     1,  1090,  2674,     8,   373,     1],\n",
      "        [  373,   812,     7,  1941,   373,  2674,  1070,  1090,     1,  2616],\n",
      "        [ 6370,   690,     1,  6370,  1070,   445,   926,  1469,     1,   926],\n",
      "        [ 2616,  1070,  1115,     1,  1090,  2674,     8,   373,     1,     3],\n",
      "        [16686,   445,  1469,  4253,     8,   812,  4253,     1,     7,   373],\n",
      "        [  926,     7,   373,  1090,     1,   812,     8,  4253,  2674,  1090],\n",
      "        [ 1090,  2674,     8,   373,     1,     8,   373,     1,     7,   926],\n",
      "        [ 1415,  1070,   966,   690,     1,  1415,  2674,   690,  1415,  3176],\n",
      "        [ 4253,  1070,  1090,     1,  1090,  2674,   690,   966,     1,  1415],\n",
      "        [ 1070,  1415,  1090,  1070,  1958,   690,  1115,     1,  6370,   690],\n",
      "        [ 2616,   690,   966,     8,   812,     8,   373,  1090,     1,  1958],\n",
      "        [ 1070,   812,   690,     1,  1070,  2616,     1,  1090,  2674,   690],\n",
      "        [ 1941,  1070,   373,  1090,   690,  1469,     1,  1958,  1118,     1],\n",
      "        [ 6370,   690,     1,  2674,     7,  3683,   690,     1,  1090,  2674],\n",
      "        [ 1941,   926,   690,     7,   373,   690,     1,   373,     7,  1118],\n",
      "        [ 1090,  2674,     8,   373,     1,     7,     8,   812,  1090,     1],\n",
      "        [ 1090,  6370,  1070,     1,  1415,  1070,   445,  1941,   926,   690],\n",
      "        [ 1070,   445,  1090,     1,  6370,     8,  1090,  2674,     1,  1090],\n",
      "        [  812,     8,  4253,  2674,  1090,     1,  3683,     8,  1958,   690],\n",
      "        [    7,   812,  1070,  1090,  2674,   690,  1115,     1,  4253,  1070],\n",
      "        [ 1469,  1070,     1,     8,  1090,     1,  2616,  1070,  1115,     1],\n",
      "        [ 1118,  1070,   445,  1090,   445,  1958,   690,  1115,     3,     1],\n",
      "        [ 1090,  2674,   690,  1115,   690,     1,     7,  1115,   690,     1],\n",
      "        [    8,     1,     7,     8,   812,  1090,     1,   373,   690,   947],\n",
      "        [ 2674,     7,  1941,  1941,  1118,     1,  1958,     8,  1115,  1090],\n",
      "        [ 1090,  2674,     8,   373,     1,  4253,   445,  1118,     1,  1090],\n",
      "        [    8,  1090,     1,  1469,  1070,   690,   373,   812,  1090,     1],\n",
      "        [ 1090,  1070,   812,     8,  4253,  2674,  1090,     3,     1,   926],\n",
      "        [ 1115,     7,  1941,   690,     3,     1,     7,  1958,   445,   373],\n",
      "        [    8,   812,     1,  1090,  2674,  1115,   690,   690,     1,  6370],\n",
      "        [    1,  1090,  1070,  1469,     7,  1118,     1,   966,     7,  1115],\n",
      "        [ 6370,  2674,   690,   812,     1,  1118,  1070,   445,     1,  4253],\n",
      "        [ 6370,  1070,   966,   690,   812,     1,     7,  1115,   690,     1],\n",
      "        [  373,  1070,     1,  1958,   690,  4253,     8,   812,     1,  1090],\n",
      "        [ 6370,  2674,     7,  1090,     1,  6370,   690,     1,   373,     7],\n",
      "        [  926,     8,  3176,   690,     1,  6370,  2674,     7,  1090,     1],\n",
      "        [  373,   445,   812,  1469,     7,  1118,     1,   966,  1070,  1115],\n",
      "        [ 1941,   445,  1090,     1,  1118,  1070,   445,  1115,     1,  2616],\n",
      "        [  373,     8,   812,  1415,   690,     1,   445,  1941,   926,  1070],\n",
      "        [ 1958,   690,   373,  1090,     1,   966,   690,   966,   690,     1],\n",
      "        [  373,  6370,     8,  1941,   690,     1,  1090,  1070,     1,   373],\n",
      "        [    8,     1,  4253,  1070,  1090,  1090,     7,     1,  1958,   690],\n",
      "        [    8,     1,  6370,  1070,   445,   926,  1469,     1,   373,     7],\n",
      "        [ 1070,   966,  4253,     2,     1,     8,  1090,   171,   373,     1],\n",
      "        [16686,     7,  1941,     1,   966,   690,  1090,  1115,  1070,  1941],\n",
      "        [ 1090,  2674,     8,   373,     1,  1415,  1115,     7,  1415,  3176],\n",
      "        [ 1958,   690,  1415,  1070,   966,   690,     1,  1070,   812,   690],\n",
      "        [    7,     1,   926,     8,  1090,  1090,   926,   690,     1,   966],\n",
      "        [ 1090,  2674,   690,  1115,   690,   171,   373,     1,   812,  1070],\n",
      "        [ 4253,  1115,     7,   812,     1,  2616,     8,   690,   373,  1090],\n",
      "        [ 6370,  1070,  6370,     1,  2674,  1070,  6370,     1,   373,   690],\n",
      "        [ 2674,     7,  1941,  1941,  1118,     1,  1090,  2674,   445,  1115],\n",
      "        [ 3683,   690,  1115,  1118,     1,  6370,   690,   926,   926,     1],\n",
      "        [ 1090,  2674,   690,     1,  6370,     7,  1118,     1,     8,   373],\n",
      "        [    8,  1941,  2674,  1070,   812,   690,     1,     8,   373,     1],\n",
      "        [    7,     1,   926,  1070,  3683,   690,   926,  1118,     1,  6370],\n",
      "        [ 2616,  1070,  1415,   445,   373,     1,  1070,   812,     1,  6370],\n",
      "        [  373,  1090,  1115,   690,   812,  4253,  1090,  2674,     1,     8],\n",
      "        [ 1090,     7,  4253,     1,     7,     1,  2616,  1115,     8,   690],\n",
      "        [ 1090,  2674,     8,   373,     1,  6370,     8,   926,   926,     1],\n",
      "        [  966,     8,   373,   373,     1,   966,  1118,     1,  1958,     7],\n",
      "        [ 1415,  2674,   690,   690,   373,   690,     1,  2616,  1070,  1115],\n",
      "        [ 6370,  2674,     7,  1090,     1,    17,     1,    17,     1,    17],\n",
      "        [ 2616,   690,   966,     8,   812,     8,   373,   966,     1,     8],\n",
      "        [    7,   373,     1,   926,  1070,   812,  4253,     1,     7,   373],\n",
      "        [    7,   812,  1469,     1,  2674,   690,  1115,   690,     1,  2674],\n",
      "        [ 1070,   445,  1115,     1,  1070,   812,   926,  1118,     1,   926],\n",
      "        [ 1090,  2674,   690,     1,   966,  1070,  3683,   690,   966,   690],\n",
      "        [ 1090,  2674,   690,     1,   373,  1070,     7,   373,     1,  1415],\n",
      "        [ 2674,   690,  1118,     1,   690,  3683,   690,  1115,  1118,  1070],\n",
      "        [ 6370,   690,  1958,   373,     1,  1415,     7,   812,     1,  4253],\n",
      "        [ 3176,     7,  2674,     8,     1,  1090,  1070,  2674,     1,  2674],\n",
      "        [ 2616,  1070,  1115,   690,  3683,   690,  1115,     1,     7,   812],\n",
      "        [    8,     1,  6370,  1070,   445,   926,  1469,     1,   926,     8],\n",
      "        [  926,   690,  1090,   171,   373,     1,  1958,   690,     1,  1415],\n",
      "        [ 2674,   690,  1115,   690,     1,     8,   373,     1,     7,     1],\n",
      "        [ 6370,  2674,  1118,     1,  2616,   690,   966,     8,   812,     8],\n",
      "        [ 6370,     7,     8,   373,  1090,     1,  1415,   926,   445,  1090],\n",
      "        [ 1469,   690,     7,  1115,     1,  3683,     8,   690,  6370,   690],\n",
      "        [ 1070,   445,  1090,   373,  1090,     7,   812,  1469,     8,   812],\n",
      "        [ 2674,     7,  1958,     8,  1090,     7,  1090,     1,  1941,  1115],\n",
      "        [ 2616,  1070,  1115,  1090,   445,   812,     7,  1090,   690,   926],\n",
      "        [ 1090,  2674,     8,   373,     1,     8,   373,     1,   373,  1070],\n",
      "        [    8,     1,   373,  6370,   690,     7,  1115,     1,     8,     1],\n",
      "        [ 6370,   690,     1,     7,  1415,  1090,   445,     7,   926,   926]])\n",
      "[tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])]\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, samples in enumerate(trainLoader):\n",
    "    print(i)\n",
    "    print(samples[0])\n",
    "    print(samples[1])\n",
    "    print(samples[2])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
